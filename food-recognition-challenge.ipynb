{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "harmful-precipitation",
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-04-25T10:08:48.855720Z",
     "iopub.status.busy": "2021-04-25T10:08:48.853907Z",
     "iopub.status.idle": "2021-04-25T10:09:07.054688Z",
     "shell.execute_reply": "2021-04-25T10:09:07.053981Z"
    },
    "papermill": {
     "duration": 18.216026,
     "end_time": "2021-04-25T10:09:07.054875",
     "exception": false,
     "start_time": "2021-04-25T10:08:48.838849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycocotools\r\n",
      "  Downloading pycocotools-2.0.2.tar.gz (23 kB)\r\n",
      "Requirement already satisfied: setuptools>=18.0 in /opt/conda/lib/python3.7/site-packages (from pycocotools) (49.6.0.post20210108)\r\n",
      "Requirement already satisfied: cython>=0.27.3 in /opt/conda/lib/python3.7/site-packages (from pycocotools) (0.29.23)\r\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from pycocotools) (3.4.1)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (2.4.7)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (0.10.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (1.3.1)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (7.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.1)\r\n",
      "Requirement already satisfied: numpy>=1.16 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (1.19.5)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools) (1.15.0)\r\n",
      "Building wheels for collected packages: pycocotools\r\n",
      "  Building wheel for pycocotools (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.2-cp37-cp37m-linux_x86_64.whl size=272449 sha256=3aba2675c63e9bce94d6b8ba0883ba7cb25db552742b2a0db5592f6d43851916\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/cf/1b/e95c99c5f9d1648be3f500ca55e7ce55f24818b0f48336adaf\r\n",
      "Successfully built pycocotools\r\n",
      "Installing collected packages: pycocotools\r\n",
      "Successfully installed pycocotools-2.0.2\r\n"
     ]
    }
   ],
   "source": [
    "# Install the pycoco library\n",
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "musical-advancement",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-25T10:09:07.093943Z",
     "iopub.status.busy": "2021-04-25T10:09:07.092921Z",
     "iopub.status.idle": "2021-04-25T10:09:09.294689Z",
     "shell.execute_reply": "2021-04-25T10:09:09.293960Z"
    },
    "papermill": {
     "duration": 2.224188,
     "end_time": "2021-04-25T10:09:09.294848",
     "exception": false,
     "start_time": "2021-04-25T10:09:07.070660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "from pycocotools.coco import COCO\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "incident-accounting",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-25T10:09:09.329421Z",
     "iopub.status.busy": "2021-04-25T10:09:09.328716Z",
     "iopub.status.idle": "2021-04-25T10:09:09.331137Z",
     "shell.execute_reply": "2021-04-25T10:09:09.331680Z"
    },
    "papermill": {
     "duration": 0.022376,
     "end_time": "2021-04-25T10:09:09.331852",
     "exception": false,
     "start_time": "2021-04-25T10:09:09.309476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the image and annotation paths\n",
    "train_imgs_path = '../input/food-recognition-challenge/train-v0.4/train/images'\n",
    "train_anns_path = '../input/food-recognition-challenge/train-v0.4/train/annotations.json'\n",
    "\n",
    "val_imgs_path = '../input/food-recognition-challenge/val-v0.4/val/images'\n",
    "val_anns_path = '../input/food-recognition-challenge/val-v0.4/val/annotations.json'\n",
    "\n",
    "test_imgs_path = '../input/food-recognition-challenge/test_images-v0.4/val/images'\n",
    "test_anns_path = '../input/food-recognition-challenge/test_images-v0.4/val/annotations.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-emission",
   "metadata": {
    "papermill": {
     "duration": 0.01411,
     "end_time": "2021-04-25T10:09:09.360502",
     "exception": false,
     "start_time": "2021-04-25T10:09:09.346392",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reference to Image Segmentation\n",
    "\n",
    "https://divamgupta.com/image-segmentation/2019/06/06/deep-learning-semantic-segmentation-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "labeled-alliance",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-25T10:09:09.392594Z",
     "iopub.status.busy": "2021-04-25T10:09:09.391868Z",
     "iopub.status.idle": "2021-04-25T10:09:09.398879Z",
     "shell.execute_reply": "2021-04-25T10:09:09.399354Z"
    },
    "papermill": {
     "duration": 0.024695,
     "end_time": "2021-04-25T10:09:09.399546",
     "exception": false,
     "start_time": "2021-04-25T10:09:09.374851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to load JSON into a COCO api\n",
    "def getCOCO(anns_path):\n",
    "    # Initialize the COCO api for instance annotations\n",
    "    coco = COCO(anns_path)\n",
    "    \n",
    "    # Load the categories in a variable\n",
    "    catIDs = coco.getCatIds()\n",
    "    cats = coco.loadCats(catIDs)\n",
    "    \n",
    "    # Print number of categories\n",
    "    nms = [cat['name'] for cat in cats]\n",
    "    print('\\nNumber of COCO categories: {}'.format(len(nms)))\n",
    "    \n",
    "    # Create a dataframe of the count of each category\n",
    "    df = pd.DataFrame(columns=['Category', 'ID', 'Count'])\n",
    "    \n",
    "    # Add each category and its count row by row\n",
    "    for i, catID in enumerate(catIDs):\n",
    "        imgIds = coco.getImgIds(catIds=catID)\n",
    "        df.loc[i] = [nms[i]] + [catID] + [len(imgIds)]     \n",
    "        \n",
    "    return coco, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "civilian-french",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-25T10:09:09.432350Z",
     "iopub.status.busy": "2021-04-25T10:09:09.431683Z",
     "iopub.status.idle": "2021-04-25T10:09:14.930602Z",
     "shell.execute_reply": "2021-04-25T10:09:14.929793Z"
    },
    "papermill": {
     "duration": 5.516562,
     "end_time": "2021-04-25T10:09:14.930771",
     "exception": false,
     "start_time": "2021-04-25T10:09:09.414209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.31s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Number of COCO categories: 273\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>ID</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>water</td>\n",
       "      <td>2578</td>\n",
       "      <td>1835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pear</td>\n",
       "      <td>1157</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>egg</td>\n",
       "      <td>2022</td>\n",
       "      <td>626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grapes</td>\n",
       "      <td>1198</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>butter</td>\n",
       "      <td>2053</td>\n",
       "      <td>1008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category    ID Count\n",
       "0    water  2578  1835\n",
       "1     pear  1157   151\n",
       "2      egg  2022   626\n",
       "3   grapes  1198    94\n",
       "4   butter  2053  1008"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the annotations of the image dataset\n",
    "coco, classes = getCOCO(train_anns_path)\n",
    "\n",
    "# Preview a sample of the classes dataframe\n",
    "classes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-custody",
   "metadata": {
    "papermill": {
     "duration": 0.01542,
     "end_time": "2021-04-25T10:09:14.962037",
     "exception": false,
     "start_time": "2021-04-25T10:09:14.946617",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reference for Getting Masks\n",
    "\n",
    "https://towardsdatascience.com/master-the-coco-dataset-for-semantic-image-segmentation-part-2-of-2-c0d1f593096a\n",
    "\n",
    "https://github.com/virafpatrawala/COCO-Semantic-Segmentation/blob/master/COCOdataset_SemanticSegmentation_Demo.ipynb\n",
    "\n",
    "https://github.com/qubvel/segmentation_models/issues/137\n",
    "\n",
    "https://www.reddit.com/r/computervision/comments/ihh8n0/onehotencoding_with_multichannel_images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aware-hypothetical",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-25T10:09:15.008053Z",
     "iopub.status.busy": "2021-04-25T10:09:15.007386Z",
     "iopub.status.idle": "2021-04-25T10:09:15.010431Z",
     "shell.execute_reply": "2021-04-25T10:09:15.009931Z"
    },
    "papermill": {
     "duration": 0.032549,
     "end_time": "2021-04-25T10:09:15.010636",
     "exception": false,
     "start_time": "2021-04-25T10:09:14.978087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getGenerator(img_folder, batch_size):\n",
    "    c = 0\n",
    "    n = os.listdir(img_folder) #List of training images\n",
    "    random.shuffle(n)\n",
    "    \n",
    "    while (True):\n",
    "        img_batch = np.zeros((batch_size, 224, 224, 3)).astype('int')\n",
    "        mask_batch = np.zeros((batch_size, 224, 224, 274)).astype('int')\n",
    "\n",
    "        for i in range(c, c + batch_size):\n",
    "            train_img = cv2.imread(img_folder + '/' + n[i])\n",
    "            train_img =  cv2.resize(train_img, (224, 224))\n",
    "            train_img = cv2.cvtColor(train_img, cv2.COLOR_BGR2RGB)\n",
    "            img_batch[i-c] = train_img\n",
    "\n",
    "            def getMask(image_id):\n",
    "                # Create a zero array with the given size and number of classes\n",
    "                mask = np.zeros((224, 224, 274))\n",
    "            \n",
    "                annIds = coco.getAnnIds(int(image_id))\n",
    "                anns = coco.loadAnns(annIds)\n",
    "                \n",
    "                for j, ann in enumerate(anns):\n",
    "                    # Get the binary mask for the annotation\n",
    "                    binary = cv2.resize(coco.annToMask(ann), (224, 224))\n",
    "\n",
    "                    # Get the channel index for the annotation\n",
    "                    channel = classes[classes.ID == ann['category_id']].index[0] + 1\n",
    "\n",
    "                    # Update the channel of the annotation\n",
    "                    mask[:, :, channel] = binary\n",
    "                    \n",
    "                    # Update the background channel of the annotation\n",
    "                    if j == 0:\n",
    "                        mask[:, :, 0] = np.logical_or(mask[:, :, 0], np.logical_not(binary))\n",
    "                    else:\n",
    "                        mask[:, :, 0] = np.logical_and(mask[:, :, 0], np.logical_not(binary))\n",
    "\n",
    "                return mask\n",
    "            \n",
    "            train_mask = getMask(n[i][1:-4])\n",
    "\n",
    "            mask_batch[i-c] = train_mask\n",
    "\n",
    "        c += batch_size\n",
    "        if(c + batch_size >= len(os.listdir(img_folder))):\n",
    "            c = 0\n",
    "            random.shuffle(n)\n",
    "\n",
    "        yield img_batch, mask_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "welcome-chick",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-25T10:09:15.047290Z",
     "iopub.status.busy": "2021-04-25T10:09:15.046628Z",
     "iopub.status.idle": "2021-04-25T10:09:15.049633Z",
     "shell.execute_reply": "2021-04-25T10:09:15.048864Z"
    },
    "papermill": {
     "duration": 0.023308,
     "end_time": "2021-04-25T10:09:15.049781",
     "exception": false,
     "start_time": "2021-04-25T10:09:15.026473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the images and masks from the train path\n",
    "train_gen = getGenerator(train_imgs_path, batch_size=10)\n",
    "\n",
    "# Get the images and masks from the validation path\n",
    "val_gen = getGenerator(val_imgs_path, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-adult",
   "metadata": {
    "papermill": {
     "duration": 0.016466,
     "end_time": "2021-04-25T10:09:15.082429",
     "exception": false,
     "start_time": "2021-04-25T10:09:15.065963",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reference for Model Building\n",
    "\n",
    "https://github.com/qubvel/segmentation_models#quick-start\n",
    "\n",
    "https://github.com/qubvel/segmentation_models/issues/374\n",
    "\n",
    "https://github.com/qubvel/segmentation_models/blob/master/examples/multiclass%20segmentation%20(camvid).ipynb\n",
    "\n",
    "https://github.com/qubvel/segmentation_models/issues/254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dressed-birth",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-04-25T10:09:15.130463Z",
     "iopub.status.busy": "2021-04-25T10:09:15.129748Z",
     "iopub.status.idle": "2021-04-25T10:09:30.439346Z",
     "shell.execute_reply": "2021-04-25T10:09:30.438775Z"
    },
    "papermill": {
     "duration": 15.339666,
     "end_time": "2021-04-25T10:09:30.439513",
     "exception": false,
     "start_time": "2021-04-25T10:09:15.099847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting segmentation_models\r\n",
      "  Downloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\r\n",
      "Collecting image-classifiers==1.0.0\r\n",
      "  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\r\n",
      "Collecting keras-applications<=1.0.8,>=1.0.7\r\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 50 kB 650 kB/s \r\n",
      "\u001b[?25hCollecting efficientnet==1.0.0\r\n",
      "  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\r\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.0.0->segmentation_models) (0.18.1)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (2.10.0)\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.19.5)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.15.0)\r\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.5)\r\n",
      "Requirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.5.4)\r\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.1.1)\r\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.9.0)\r\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2021.4.8)\r\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (3.4.1)\r\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (7.2.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (2.4.7)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (1.3.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (2.8.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (0.10.0)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0->segmentation_models) (4.4.2)\r\n",
      "Installing collected packages: keras-applications, image-classifiers, efficientnet, segmentation-models\r\n",
      "Successfully installed efficientnet-1.0.0 image-classifiers-1.0.0 keras-applications-1.0.8 segmentation-models-1.0.1\r\n",
      "env: SM_FRAMEWORK=tf.keras\n",
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "# Install and import the segmentation models library\n",
    "!pip install segmentation_models\n",
    "%env SM_FRAMEWORK=tf.keras\n",
    "import segmentation_models as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "equivalent-singing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-25T10:09:30.489489Z",
     "iopub.status.busy": "2021-04-25T10:09:30.488723Z",
     "iopub.status.idle": "2021-04-25T10:09:37.320399Z",
     "shell.execute_reply": "2021-04-25T10:09:37.319647Z"
    },
    "papermill": {
     "duration": 6.858769,
     "end_time": "2021-04-25T10:09:37.320551",
     "exception": false,
     "start_time": "2021-04-25T10:09:30.461782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet34_imagenet_1000_no_top.h5\n",
      "85524480/85521592 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Define the model to be trained\n",
    "model = sm.Unet('resnet34', encoder_weights='imagenet', classes=274, activation='softmax')\n",
    "model.compile('Adam', loss='categorical_crossentropy', metrics=[sm.metrics.iou_score, sm.metrics.precision, sm.metrics.recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "secret-treasurer",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-04-25T10:09:37.420080Z",
     "iopub.status.busy": "2021-04-25T10:09:37.419247Z",
     "iopub.status.idle": "2021-04-25T10:18:54.254398Z",
     "shell.execute_reply": "2021-04-25T10:18:54.253390Z"
    },
    "papermill": {
     "duration": 556.887639,
     "end_time": "2021-04-25T10:18:54.254671",
     "exception": false,
     "start_time": "2021-04-25T10:09:37.367032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10/10 [==============================] - 126s 11s/step - loss: 5.2299 - iou_score: 1.7665e-04 - precision: 0.0037 - recall: 0.9433\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 106s 11s/step - loss: 4.2259 - iou_score: 3.5051e-04 - precision: 0.0044 - recall: 0.9457\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 106s 11s/step - loss: 3.0664 - iou_score: 0.0011 - precision: 0.0045 - recall: 0.9428\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 105s 11s/step - loss: 2.6609 - iou_score: 0.0020 - precision: 0.0051 - recall: 0.9465\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 110s 11s/step - loss: 1.8844 - iou_score: 0.0022 - precision: 0.0045 - recall: 0.9543\n"
     ]
    }
   ],
   "source": [
    "# Train the defined model on the dataset\n",
    "history = model.fit(train_gen, epochs=5, steps_per_epoch=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 617.199256,
   "end_time": "2021-04-25T10:18:57.297991",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-25T10:08:40.098735",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
