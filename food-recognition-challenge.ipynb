{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install the pycoco library\n!pip install pycocotools","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"papermill":{"duration":17.136133,"end_time":"2021-04-05T06:41:14.377452","exception":false,"start_time":"2021-04-05T06:40:57.241319","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport random\nimport cv2\nfrom pycocotools.coco import COCO\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport imgaug as ia\nimport imgaug.augmenters as iaa","metadata":{"papermill":{"duration":2.233957,"end_time":"2021-04-05T06:41:16.630936","exception":false,"start_time":"2021-04-05T06:41:14.396979","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the image and annotation paths\ntrain_imgs_path = '../input/food-recognition-challenge/train-v0.4/train/images'\ntrain_anns_path = '../input/food-recognition-challenge/train-v0.4/train/annotations.json'\n\nval_imgs_path = '../input/food-recognition-challenge/val-v0.4/val/images'\nval_anns_path = '../input/food-recognition-challenge/val-v0.4/val/annotations.json'\n\ntest_imgs_path = '../input/food-recognition-challenge/test_images-v0.4/val/images'\ntest_anns_path = '../input/food-recognition-challenge/test_images-v0.4/val/annotations.json'","metadata":{"papermill":{"duration":0.026806,"end_time":"2021-04-05T06:41:16.67638","exception":false,"start_time":"2021-04-05T06:41:16.649574","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reference to Image Segmentation\n\nhttps://divamgupta.com/image-segmentation/2019/06/06/deep-learning-semantic-segmentation-keras.html","metadata":{"papermill":{"duration":0.018781,"end_time":"2021-04-05T06:41:16.714254","exception":false,"start_time":"2021-04-05T06:41:16.695473","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Function to load JSON into a COCO api\ndef getCOCO(anns_path):\n    # Initialize the COCO api for instance annotations\n    coco = COCO(anns_path)\n    \n    # Load the categories in a variable\n    catIDs = coco.getCatIds()\n    cats = coco.loadCats(catIDs)\n    \n    # Print number of categories\n    nms = [cat['name'] for cat in cats]\n    print('\\nNumber of COCO categories: {}'.format(len(nms)))\n    \n    # Create a dataframe of the count of each category\n    df = pd.DataFrame(columns=['Category', 'Count'])\n    \n    # Add each category and its count row by row\n    for i, catID in enumerate(catIDs):\n        imgIds = coco.getImgIds(catIds=catID)\n        df.loc[i] = [nms[i]] + [len(imgIds)]\n        \n    # Sort the values in descending order of count\n    df = df.sort_values('Count', ascending=False).reset_index(drop=True)        \n        \n    return coco, df","metadata":{"papermill":{"duration":0.030909,"end_time":"2021-04-05T06:41:16.76448","exception":false,"start_time":"2021-04-05T06:41:16.733571","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the annotations of the image dataset\ncoco, classes = getCOCO(train_anns_path)\n\n# Preview a sample of the classes dataframe\nclasses.head()","metadata":{"papermill":{"duration":1.104816,"end_time":"2021-04-05T06:41:17.888484","exception":false,"start_time":"2021-04-05T06:41:16.783668","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reference for Getting Masks\n\nhttps://towardsdatascience.com/master-the-coco-dataset-for-semantic-image-segmentation-part-2-of-2-c0d1f593096a","metadata":{"papermill":{"duration":0.020591,"end_time":"2021-04-05T06:41:17.9291","exception":false,"start_time":"2021-04-05T06:41:17.908509","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Function that gets images. \n# It is useful possible to get just a subset of images by setting subset to True. This option\n# can be useful for example to make first attempts of training without using the whole training set\ndef getImgs(path, image_size, subset=False, subset_size=500):\n    # Get all image Ids\n    imgIds = coco.getImgIds()\n    images = coco.loadImgs(imgIds)\n    \n    if subset:\n        images = random.sample(images, subset_size)\n    \n    # Get all annotation Ids\n    annIds = coco.getAnnIds()\n    anns = coco.loadAnns(annIds)\n    \n    img_lst = []\n    msk_lst = []\n    obj_lst = []\n    \n    for file in images:\n        # Read the image from the full file path\n        img = cv2.imread(path + '/' + file['file_name'])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # Resize the image to the given size\n        img = cv2.resize(img, image_size)\n        \n        # Create a zero array for the mask with the given size\n        full_mask = np.zeros(image_size)\n        \n        # Create a list to save the name of each object of the mask\n        obj_names = []\n        \n        for i, ann in enumerate(anns):\n            # Check if the annotation belongs to the current image\n            if ann['image_id'] == file['id']:\n                annotation = anns.pop(i)\n                \n                # Get the category's name from the annotation's category Id\n                category = coco.loadCats(annotation['category_id'])[0]['name']\n                obj_names.append(category)\n                \n                # Label each object in the mask with its pixel value from the classes dataframe\n                # The values are incremented by 1 so that only the background (no object) has a pixel value of 0\n                mask = coco.annToMask(annotation) * (classes[classes.Category == category].index[0] + 1)\n                \n                # Resize and merge the mask for each object in the image\n                mask = cv2.resize(mask, image_size)\n                full_mask = np.maximum(mask, full_mask)\n        \n        # Normalise the image and mask to a value between 0 and 1\n        norm_img = cv2.normalize(img, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n        norm_mask = cv2.normalize(full_mask, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n        \n        img_lst.append(norm_img)\n        msk_lst.append(norm_mask)\n        obj_lst.append(obj_names)\n                \n    return img_lst, msk_lst, obj_lst","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the images and masks from the given path\nimages, masks, objects = getImgs(train_imgs_path, (224, 224), subset=True, subset_size=500)\n\n# Check if the number of images and masks are the same\nprint('Number of images: {}'.format(len(images)))\nprint('Number of masks: {}'.format(len(masks)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to show a sample image with its mask\ndef showSamp(imgs, msks, objs, n=None):\n    plt.figure(figsize=(10, 10))\n    length = len(imgs)\n    \n    if n is None:\n        n = random.randrange(length)\n    \n    ax1 = plt.subplot(1, 2, 1)\n    ax1.imshow(imgs[n])\n    ax1.axis('off')\n    ax1.title.set_text('Sample Image')\n    \n    ax2 = plt.subplot(1, 2, 2)\n    ax2.imshow(msks[n])\n    ax2.axis('off')\n    ax2.title.set_text('Sample Mask')\n    \n    for i, e in enumerate(objs[n]):\n        if i == 0:\n            objects = e\n        else:\n            objects += ', ' + e\n    print('Objects: {}'.format(objects))\n    \n    plt.show()","metadata":{"papermill":{"duration":0.032169,"end_time":"2021-04-05T06:41:53.985526","exception":false,"start_time":"2021-04-05T06:41:53.953357","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show sample images and masks\nshowSamp(images, masks, objects)","metadata":{"papermill":{"duration":0.265837,"end_time":"2021-04-05T06:41:54.272248","exception":false,"start_time":"2021-04-05T06:41:54.006411","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reference for Augmentation\n\nhttps://github.com/aleju/imgaug","metadata":{"papermill":{"duration":0.025165,"end_time":"2021-04-05T06:41:54.32297","exception":false,"start_time":"2021-04-05T06:41:54.297805","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Function to augment the images and masks\ngeo_seq = iaa.Sequential([\n    iaa.Fliplr(1), # horizontally flip the images\n])\n\nint_seq = iaa.Sequential([\n    iaa.GaussianBlur(sigma=(0, 5.0)) # blur images with a sigma of 0 to 5.0\n])\n\ndef augment_img(img, msk):\n    geo_aug = geo_seq.to_deterministic()\n    int_aug = int_seq.to_deterministic()\n    \n    # Apply geographic augmentation to image and mask\n    img_aug = geo_aug.augment_image(img)\n    msk_aug = geo_aug.augment_image(msk)\n    \n    # Apply intensity augmentation to image only\n    img_aug = int_aug.augment_image(img_aug) \n    \n    return img_aug, msk_aug","metadata":{"papermill":{"duration":0.036025,"end_time":"2021-04-05T06:41:54.384398","exception":false,"start_time":"2021-04-05T06:41:54.348373","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to show an image and a mask along with their augments\ndef showAug(imgs, msks, n=None):\n    plt.figure(figsize=(10, 10))\n    length = len(imgs)\n    \n    if n is None:\n        n = random.randrange(length)\n        \n    img_aug, msk_aug = augment_img(imgs[n], msks[n])\n    \n    ax1 = plt.subplot(2, 2, 1)\n    ax1.imshow(imgs[n])\n    ax1.axis('off')\n    ax1.title.set_text('Original Image')\n    \n    ax2 = plt.subplot(2, 2, 2)\n    ax2.imshow(img_aug)\n    ax2.axis('off')\n    ax2.title.set_text('Augmented Image')\n    \n    ax3 = plt.subplot(2, 2, 3)\n    ax3.imshow(msks[n])\n    ax3.axis('off')\n    ax3.title.set_text('Original Mask')\n    \n    ax4 = plt.subplot(2, 2, 4)\n    ax4.imshow(msk_aug)\n    ax4.axis('off')\n    ax4.title.set_text('Augmented Mask')    \n    \n    plt.show()","metadata":{"papermill":{"duration":0.037676,"end_time":"2021-04-05T06:41:54.447608","exception":false,"start_time":"2021-04-05T06:41:54.409932","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show sample images and masks with their augments\nshowAug(images, masks)","metadata":{"papermill":{"duration":0.42548,"end_time":"2021-04-05T06:41:54.89873","exception":false,"start_time":"2021-04-05T06:41:54.47325","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Function to create and shuffle a list of original and augmented images and masks\ndef getAugs(imgs, msks, K):\n    length = len(imgs)\n    \n    img_lst = imgs.copy()\n    msk_lst = msks.copy()\n    \n    for i in range(K):\n        n = random.randrange(length)\n        img_aug, msk_aug = augment_img(imgs[n], msks[n])\n        \n        img_lst.append(img_aug)\n        msk_lst.append(msk_aug)\n    \n    # Shuffle all the original and augmented images and masks\n    z = list(zip(img_lst, msk_lst))\n    random.shuffle(z)\n    img_lst, msk_lst = zip(*z)\n    \n    return img_lst, msk_lst","metadata":{"papermill":{"duration":0.041642,"end_time":"2021-04-05T06:41:54.972983","exception":false,"start_time":"2021-04-05T06:41:54.931341","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Augment a given number of images and masks\naug_images, aug_masks = getAugs(images, masks, 200)\n\n# Show the number of augmented images and masks\nprint('Number of augmented images: {}'.format(len(aug_images)))\nprint('Number of augmented masks: {}'.format(len(aug_masks)))","metadata":{"papermill":{"duration":0.181794,"end_time":"2021-04-05T06:41:55.187223","exception":false,"start_time":"2021-04-05T06:41:55.005429","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reference for Model Building\n\nhttps://github.com/qubvel/segmentation_models#quick-start\n\nhttps://github.com/qubvel/segmentation_models/issues/374","metadata":{"papermill":{"duration":0.032477,"end_time":"2021-04-05T06:41:55.252524","exception":false,"start_time":"2021-04-05T06:41:55.220047","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Install and import the segmentation models library\n!pip install segmentation_models\n%env SM_FRAMEWORK=tf.keras\nimport segmentation_models as sm","metadata":{"_kg_hide-output":true,"papermill":{"duration":13.771542,"end_time":"2021-04-05T06:42:09.056718","exception":false,"start_time":"2021-04-05T06:41:55.285176","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the model to be trained\nBACKBONE = 'resnet34'\nmodel = sm.Unet(BACKBONE, encoder_weights='imagenet')\nmodel.compile('Adam', loss=sm.losses.bce_jaccard_loss, metrics=[sm.metrics.iou_score])","metadata":{"papermill":{"duration":6.077549,"end_time":"2021-04-05T06:42:15.173923","exception":false,"start_time":"2021-04-05T06:42:09.096374","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the defined model on the dataset\nhistory = model.fit(x=np.array(aug_images), y=np.array(aug_masks), batch_size=16, epochs=10)","metadata":{"papermill":{"duration":7411.546199,"end_time":"2021-04-05T08:45:46.782867","exception":false,"start_time":"2021-04-05T06:42:15.236668","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the loss and iou score over the epochs\nfig = plt.figure(figsize=(16, 8))\nax1 = fig.add_subplot(111)\nax2 = ax1.twinx()\n\np1, = ax1.plot(history.history['iou_score'], 'g')\nax1.set_ylabel('IOU Score')\n\np2, = ax2.plot(history.history['loss'], 'r')\nax2.set_ylabel('Loss')\n\nax1.legend([p1, p2], ['IOU Score', 'Loss'], loc='upper center')\n\nplt.title('Model History Over Epochs')\nplt.xlabel('Epochs')\n\nplt.show()","metadata":{"papermill":{"duration":0.624407,"end_time":"2021-04-05T08:45:47.71833","exception":false,"start_time":"2021-04-05T08:45:47.093923","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model to the output\nmodel.save_weights('trained_model.h5')","metadata":{"papermill":{"duration":1.051097,"end_time":"2021-04-05T08:45:49.08085","exception":false,"start_time":"2021-04-05T08:45:48.029753","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Load the trained model weights\n# weights_path = '../input/food-recognition-model/trained_model.h5'\n# model.load_weights(weights_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}